{"cells":[{"cell_type":"markdown","source":"# Week 1 Cheatsheet\n---","metadata":{"tags":[],"cell_id":"00001-64d65af6-e208-4931-91c4-eb7e7ca6b434"}},{"cell_type":"markdown","source":"## Convert df to array\n```\n    import pandas as pd\n    data_array = pd.DataFrame.to_numpy(data)    # <- where data is pandas DataFrame\n```\nWhen assigning X and y ensure that both are of type 'float'\n```\n    X.astype('float')\n    Y.astype('float)\n```","metadata":{"tags":[],"cell_id":"00001-52953b40-b897-426c-b8e7-51bf96875451"}},{"cell_type":"markdown","source":"## Mean Squared Error Cost Function (linear reg)\n$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$","metadata":{"tags":[],"cell_id":"00001-9667f00a-467e-4ece-9805-0b4b63b7f094"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-27c9c57c-39af-4e8f-8039-94e9d886326d"},"source":"def computeCost(X, y, theta):\n    \"\"\"\n    Vectorized code to compute cost for linear regression\n    Computes the cost of using theta as the parameter for linear regression to fit the data points in X and y.\n    \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n+1).\n    \n    y : array_like\n        A vector of shape (m, ) for the values at a given data point.\n    \n    theta : array_like\n        The linear regression parameters. A vector of shape (n+1, )\n    \n    Returns\n    -------\n    J : float\n        The value of the cost function. \n    \"\"\"\n    m = y.shape[0]      # number of training examples\n    \n    J = 0\n    \n    # ======================= YOUR CODE HERE ===========================\n    h = np.dot(X, theta)\n    error = h - y\n\n    J += (1/(2*m)) * ((error**2).sum())\n    \n    # ==================================================================\n    return J","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gradient Descent (linear reg)\n_repeat to convergence {_  \n$$ \\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$  \n_}_","metadata":{"tags":[],"cell_id":"00003-14dc26c1-4b61-4cd7-ae06-bb92774fee63"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-745850a0-97df-4206-87e1-39537f5b335a"},"source":"def gradientDescent(X, y, theta):\n    \"\"\"\n    Vectorized code to perform gradient descent to best theta values.\n    Updates theta by taking num_iters gradient steps with learning rate alpha.\n\n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n+1).\n    \n    y : array_like\n        A vector of shape (m, ) for the values at a given data point.\n    \n    theta : array_like\n        The linear regression parameters. A vector of shape (n+1, )\n    \n    alpha : float\n        The learning rate for gradient descent. \n    \n    num_iters : int\n        The number of iterations to run gradient descent. \n    \n    Returns\n    -------\n    best_theta : array_like\n        The learned, \"best\" linear regression parameters. A vector of shape (n+1, ).\n    \n    J_history : list\n        A python list for the values of the cost function after each iteration. Uses computeCost() to calculate J.\n    \"\"\"\n\n    m = y.shape[0]     # number of training examples\n    \n    # make a copy of theta, so as to not directly alter initialized values\n    theta = theta.copy()\n    \n    J_history = []\n    \n    for i in range(num_iters):\n        h = np.dot(X, theta)\n        error = h - y\n        delta = (1/m) * np.dot(error.transpose(), X)\n        change = alpha * delta\n        theta -= change.transpose()\n        \n        # save the cost J in every iteration\n        J_history.append(computeCost(X, y, theta))\n    \n    return theta, J_history","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Selecting alpha","metadata":{"tags":[],"cell_id":"00005-26b3efaf-d996-44f5-97d3-eccc028a7364"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-33a2647a-4505-49fb-979c-a9be2e9710de"},"source":"alpha = [.001, .003, .01, .03, .1, .3]\n\nn = X.shape[1]    # number features \nnum_iters = 50\n\nfor num in alpha:\n    # conduct gradient descent using gradientDescent()\n    best_theta, J_history = gradientDescent(X=X, y=y, theta=np.zeros((n)), alpha=num, num_iters=num_iters)    # <- using feat. normalized X\n\n    # DEBUGGING; plots curve of cost vs. number iterations using J_history values\n    plt.plot(range(num_iters), J_history, '-', label=num)\n    plt.xlabel(\"# iterations\")\n    plt.ylabel(\"Cost (J)\")\n    plt.legend()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Normalization Example (mean normalization)\n**After** performing feature normalization, reassign x_0 values (vector of 1's) and normalized values to X:\n```\n    m = X.shape[0]\n    X = np.concatenate(np.ones(m), X_norm, axis=1)\n```","metadata":{"tags":[],"cell_id":"00007-73a95443-da07-4aff-8ef2-c995f72edb54"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00008-a656b856-e024-41bd-be31-08ee1959e32c"},"source":"def featureNormalize(X):\n    \"\"\"\n    Vectorized code performs mean normalization, (x-mu)/s, on the features in X. Returns a normalized version of X where\n    the mean value of each feature is 0 and the standard deviation is 1.\n    \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n).\n    \n    Returns\n    -------\n    X_norm : array_like\n        The normalized dataset of shape (m x n).\n    \n    mu : array_like\n        A vector of shape (n, ) for the mean values of each feature.\n\n    sigma : array_like\n        A vector of of shape (n, ) for standard deviation values of each feature.\n    \"\"\"\n\n    X_norm = X.copy()    # make a copy of X to avoid directly altering original data\n\n    mu = np.mean(X_norm, axis=0)     # average the rows of each column\n    sigma = np.std(X_norm, axis=0)  \n\n    m = X_norm.shape[0]\n\n    full_mu = np.outer(mu, np.ones((1, m))).transpose()        \n    full_sigma = np.outer(sigma, np.ones((1, m))).transpose()\n\n    X_norm = np.divide((X_norm - full_mu), full_sigma)\n    \n    return X_norm, mu, sigma\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normal Equation \n_Analytically solve for best theta values for linear reg_  \n$$ \\theta = \\left( X^T X\\right)^{-1} X^Ty$$","metadata":{"tags":[],"cell_id":"00010-c04363b3-a897-477c-835f-c3c8282f0e80"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00010-ad9e41cf-d2fb-4dc9-8ff3-3449731358e9"},"source":"def normalEqn(X, y):\n    \"\"\"\n    Vectorized code computes the closed-form solution to linear regression using the normal equations.\n    \n    Parameters\n    ----------\n    X : array_like\n        The dataset of shape (m x n+1).\n    \n    y : array_like\n        The value at each data point. A vector of shape (m, ).\n    \n    Returns\n    -------\n    theta : array_like\n        Estimated linear regression parameters. A vector of shape (n+1, ).\n    \n    \"\"\"\n    theta = np.zeros(X.shape[1])\n    \n    # ===================== YOUR CODE HERE ============================\n    theta += np.dot(np.linalg.pinv(np.dot(X.transpose(), X)), np.dot(X.transpose(), y))\n    \n    # =================================================================\n    return theta","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plotting h(x) regression line","metadata":{"tags":[],"cell_id":"00011-d5923de7-a726-4864-8008-c30c9ae9dbe6"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-ac1cb5df-b073-408f-b4c9-9573f3380908"},"source":"# plot data points\nplt.plot(x, y)\n\n# plot regression line\nplt.plot(X[:, 1], np.dot(X, theta), '-')     # np.dot(X, theta) => h(x); input data is X[:, 1]; don't include feature0 here\nplt.legend(['Training data', 'Linear regression'])  \n\npyplot.show()","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Make predictions using h(x)\nRecall $$ h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_n x_n$$","metadata":{"tags":[],"cell_id":"00011-302e3c7f-a7cf-4a15-a007-8965690d9508"}},{"cell_type":"markdown","source":"#### a) without normalization","metadata":{"tags":[],"cell_id":"00014-4c7284cc-9c1f-4b5f-8181-44e0eed43e91"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00012-37e4eb50-9137-4580-a67b-eb93a947c537"},"source":"Input = [1, x_1, x_2, ...]      # <- where 1, x_1, ... are input values for feature0, feature1, ... \n\ny_hat = np.dot(Input, theta)    # <- where theta is vector [theta_0, theta_1, theta_2, ...]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### b) with normalization example (mean normalization)","metadata":{"tags":[],"cell_id":"00016-6fa12a55-5394-448d-80b0-ddcb1b0d04b5"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00013-7ea79bf8-7c95-4d93-a945-323cbcacceb5"},"source":"prenormInput = [x_1, x_2, ...]\npostnormInput = np.divide((prenormInput - mu), sigma)   # <- where mu and sigma are vectors returned by featureNormalize()\n\nInput = np.append([1], postnormInput)                   # <- append normalized inputs to 1\n\ny_hat =  np.dot(Input, theta)","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"9c34132c-096a-4808-aa38-24f7f9ead5b4","deepnote_execution_queue":[]}}